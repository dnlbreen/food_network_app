{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from food_network_wrapper import recipe_search, get_n_recipes, scrape_recipe\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns  # plots are prettier with Seaborn\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import Image\n",
    "from IPython import display\n",
    "matplotlib.rcParams['savefig.dpi'] = 2 * matplotlib.rcParams['savefig.dpi']\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "import simplejson  # more efficient than the default json library\n",
    "import sys\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize.casual   import (TweetTokenizer, casual_tokenize)\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from itertools import islice, chain\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use food network wrapper to scrape data\n",
    "\n",
    "def search_recipe(search_term, n=10):\n",
    "    \n",
    "    recipe_list = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    rthumbnails = get_n_recipes(search_term, n=n)\n",
    "    recipes = []\n",
    "    \n",
    "    for i in rthumbnails:\n",
    "        \n",
    "        recipe_element = {}\n",
    "        \n",
    "        recipe_element['type'] = search_term\n",
    "        \n",
    "        recipe_element['title'] = i.title\n",
    "        recipe_element['url'] = i.url\n",
    "        recipe_element['author'] = i.author\n",
    "        recipe_element['picture_url'] = i.picture_url\n",
    "        recipe_element['total_time'] = i.total_time\n",
    "        recipe_element['rating'] = i.rating\n",
    "        recipe_element['review_count'] = i.review_count\n",
    "\n",
    "        recipe = scrape_recipe(i.url)\n",
    "        recipe_element['prep_time'] = recipe.prep_time\n",
    "        recipe_element['cook_time'] = recipe.cook_time\n",
    "        recipe_element['servings'] = recipe.servings\n",
    "        recipe_element['level'] = recipe.level\n",
    "        recipe_element['ingredients'] = recipe.ingredients\n",
    "        recipe_element['directions'] = recipe.directions\n",
    "        recipe_element['categories'] = recipe.categories\n",
    "        \n",
    "        recipe_list.append(recipe_element)\n",
    "    \n",
    "    df = pd.DataFrame(recipe_list)\n",
    "\n",
    "    if os.path.exists('recipes.csv'):\n",
    "\n",
    "        df.to_csv('recipes.csv', header = False, mode = 'a')\n",
    "    else:\n",
    "        df.to_csv('recipes.csv')\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scrape the data and save it to csv file as it comes in\n",
    "#limit to 200, not sure enough distinct search results are available for more\n",
    "search_recipe(\"mexican\", 200)\n",
    "search_recipe(\"polish\", 200)\n",
    "search_recipe(\"chinese\", 200)\n",
    "search_recipe(\"italian\", 200)\n",
    "search_recipe(\"japanese\", 200)\n",
    "search_recipe(\"korean\", 200)\n",
    "search_recipe(\"vietnamese\", 200)\n",
    "search_recipe(\"indian\", 200)\n",
    "search_recipe(\"thai\", 200)\n",
    "search_recipe(\"greek\", 200)\n",
    "search_recipe(\"french\", 200)\n",
    "search_recipe(\"german\", 200)\n",
    "search_recipe(\"british\", 200)\n",
    "search_recipe(\"caribbean\", 200)\n",
    "search_recipe(\"western\", 200)\n",
    "search_recipe(\"eastern\", 200)\n",
    "search_recipe(\"american\", 200)\n",
    "search_recipe(\"mediterranean\", 200)\n",
    "search_recipe(\"spanish\", 200)\n",
    "search_recipe(\"turkish\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scrape additional data based on the titles obtained from the search categories above, to be used later\n",
    "#while this data is being scraped I analyze a copy of the original\n",
    "#recipes_small.csv\n",
    "recipe_list = pd.read_csv('recipes.csv')\n",
    "\n",
    "for title in recipe_list['title'][::-1]:\n",
    "    \n",
    "    if title in types_searched:\n",
    "        print title, \" already searched, continuing\"\n",
    "        continue\n",
    "    \n",
    "    search_recipe(str(title), n = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#analyze the data, first pass is to plot wordclouds and bargraphs\n",
    "#for each category of food\n",
    "\n",
    "def nlargest(n, word_scores):\n",
    "    \"\"\" Wrapper around heapq to return the n words with the largest count.\"\"\"\n",
    "\n",
    "    return heapq.nlargest(n, word_scores, key=lambda x: x[1])\n",
    "\n",
    "def plot_gramcloud(gramcloud, cuisine_type):\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(gramcloud)\n",
    "    plt.title(cuisine_type)\n",
    "    plt.savefig('{0}_wordcloud.png'.format(cuisine_type))\n",
    "    plt.close()\n",
    "    \n",
    "def create_bargraph(counter, cuisine_type):\n",
    "    df = pd.DataFrame(nlargest(10, counter.items()), columns=['ingredients', 'count'])\n",
    "    df.set_index('ingredients').plot(kind='bar')\n",
    "    plt.title(cuisine_type)\n",
    "    plt.ylabel('occurrences')\n",
    "    plt.savefig('{0}_bargraph.png'.format(cuisine_type))\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def visualize_data(df, cuisine_type):    \n",
    "    \n",
    "    keywords = [item for sublist in df['keywords'] for item in sublist]\n",
    "    counter = Counter(keywords)\n",
    "    wordcloud = WordCloud().fit_words(counter.items())\n",
    "    plot_gramcloud(wordcloud, cuisine_type)\n",
    "    \n",
    "    create_bargraph(counter, cuisine_type)\n",
    "    \n",
    "    return counter, wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('recipes.csv')\n",
    "\n",
    "cuisine_types = set([\"mexican\", \"polish\", \"chinese\", \"italian\", \"japanese\", \"korean\", \"vietnamese\", \"indian\", \"thai\", \"greek\"\n",
    "                    ,\"french\", \"german\", \"british\", \"caribbean\", \"western\", \"eastern\", \"american\", \"mediterranean\",\n",
    "                    \"spanish\", \"turkish\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a list of stopwords manually by looking at the ingredients\n",
    "\n",
    "stop = []\n",
    "file = open('stopwords.txt')\n",
    "\n",
    "for n, line in enumerate(file):\n",
    "    stop.append(line.strip('\\n'))\n",
    "\n",
    "split_words = '\\(.*?\\)|' + '|'.join(stop)  + '|\\[|\\]|' + '1/2|1/3|2/3|1/4|3/4|1/|3/|2/|/*'\n",
    "\n",
    "#set notation is convenient to add other stop words\n",
    "#to take care of stuff not addressed by first cleaning\n",
    "#for example, I might now want to remove spices to\n",
    "#get a different picture of how ethnic cuisines differ\n",
    "\n",
    "set_remove = set(['1/', '2/', '3/', '[', ']', '', ' ', \n",
    "            'and', 'to', 'can','hot','cold','with','more', 'or', 'by', 'a', 'an', \n",
    "            '1', '2', '3', '4', '5', '6', '7', '8', '9', 'head', 'tail','.','!',',', \n",
    "            's','p','semi','-','un'])\n",
    "set_remove_spices = set(['garlic', 'salt', 'kosher salt', 'black pepper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['keywords'] = df['ingredients'].apply(lambda x: list(set( [item.strip() for item in re.split(split_words, x.lower())] ) - set_remove - set_remove_spices) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the figures for each cuisine type\n",
    "for cuisine_type in set(df['type']):\n",
    "    \n",
    "    type_df = df[df['type'] == cuisine_type]\n",
    "    counter, wordcloud = visualize_data(type_df, cuisine_type)\n",
    "    \n",
    "    counters.update({cuisine_type: counter})\n",
    "    wordclouds.update({cuisine_type: wordcloud})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "plt.title(\"With Spices\")\n",
    "count = 0\n",
    "for cuisine_type in set(df['type']):\n",
    "    count += 1\n",
    "    plt.subplot(5,4,count)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wordclouds[cuisine_type])\n",
    "    plt.title(cuisine_type, fontsize = 40)\n",
    "\n",
    "plt.savefig('withspices_wordcloud.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_even = df[df['type'].apply(lambda x: True if x in cuisine_types else False)]\n",
    "#to analyze a balanced subset of each kind of cuisine\n",
    "df_even['keywords'] = df_even['ingredients'].apply(lambda x: list(set( [item.strip() for item in re.split(split_words, x.lower())] ) - set_remove - set_remove_spices) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_even = df_even['keywords']\n",
    "\n",
    "dictionary_even = corpora.Dictionary(texts_even)\n",
    "corpus_even = [dictionary_even.doc2bow(text) for text in texts_even]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train an lda model with 20 topics and 20 passes through\n",
    "#the corpus\n",
    "alpha = 0.001\n",
    "num_topics = 20\n",
    "passes = 20\n",
    "\n",
    "ldamodel_even_20 = gensim.models.ldamodel.LdaModel(\n",
    "    corpus_even, num_topics=num_topics, id2word = dictionary_even, \n",
    "    passes=passes, alpha = alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel_even_20.save('20_topics_wos_model_even.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuisines_data_even = pyLDAvis.gensim.prepare(ldamodel_even_20, corpus_even, dictionary_even)\n",
    "\n",
    "pyLDAvis.display(cuisines_data_even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(cuisines_data_even, '20_topics_low_alpha.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_models_even_low_alpha = []\n",
    "num_topics = [3, 4, 7, 10]\n",
    "for num_topic in num_topics:\n",
    "    print \"Training Model #\", num_topic\n",
    "    alpha = 0.001\n",
    "    passes = 20 #number of passes over the supplied corpus\n",
    "\n",
    "    lda_models_even_low_alpha.append(gensim.models.ldamodel.LdaModel(\n",
    "        corpus_even, num_topics=num_topic, id2word = dictionary_even, \n",
    "        passes=passes, alpha = alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_models_even_low_alpha[0].save('3_topics_wos_model_even.lda')\n",
    "lda_models_even_low_alpha[1].save('4_topics_wos_model_even.lda')\n",
    "lda_models_even_low_alpha[2].save('7_topics_wos_model_even.lda')\n",
    "lda_models_even_low_alpha[3].save('10_topics_wos_model_even.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuisines_data_even_7 = pyLDAvis.gensim.prepare(lda_models_even_low_alpha[2], corpus_even, dictionary_even)\n",
    "\n",
    "pyLDAvis.display(cuisines_data_even_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(cuisines_data_even_7, '7_topics_low_alpha.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cuisines_data_even_4 = pyLDAvis.gensim.prepare(lda_models_even_low_alpha[1], corpus_even, dictionary_even)\n",
    "\n",
    "pyLDAvis.display(cuisines_data_even_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(cuisines_data_even_4, '4_topics_low_alpha.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
